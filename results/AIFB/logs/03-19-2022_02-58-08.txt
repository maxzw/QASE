03/19/2022 02:58:08 |  root |  INFO | Training on dataset: AIFB
03/19/2022 02:58:08 | models |  INFO | Created entity embeddings: Embedding(2601, 128)
03/19/2022 02:58:08 | models |  INFO | Created variable embeddings: Embedding(6, 128)
03/19/2022 02:58:08 | models |  INFO | Created relation embeddings: Embedding(49, 128)
03/19/2022 02:58:08 |  root |  INFO | Model: HypewiseGCN(
  (ent_features): Embedding(2601, 128)
  (var_features): Embedding(6, 128)
  (rel_features): Embedding(49, 128)
  (submodels): ModuleList(
    (0): GCNModel(
      (layers): ModuleList(
        (0): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
        (1): ReLU()
        (2): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
        (3): ReLU()
        (4): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
      )
      (pool): TargetPooling()
    )
    (1): GCNModel(
      (layers): ModuleList(
        (0): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
        (1): ReLU()
        (2): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
        (3): ReLU()
        (4): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
      )
      (pool): TargetPooling()
    )
    (2): GCNModel(
      (layers): ModuleList(
        (0): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
        (1): ReLU()
        (2): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
        (3): ReLU()
        (4): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
      )
      (pool): TargetPooling()
    )
    (3): GCNModel(
      (layers): ModuleList(
        (0): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
        (1): ReLU()
        (2): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
        (3): ReLU()
        (4): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
      )
      (pool): TargetPooling()
    )
    (4): GCNModel(
      (layers): ModuleList(
        (0): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
        (1): ReLU()
        (2): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
        (3): ReLU()
        (4): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
      )
      (pool): TargetPooling()
    )
    (5): GCNModel(
      (layers): ModuleList(
        (0): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
        (1): ReLU()
        (2): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
        (3): ReLU()
        (4): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
      )
      (pool): TargetPooling()
    )
    (6): GCNModel(
      (layers): ModuleList(
        (0): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
        (1): ReLU()
        (2): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
        (3): ReLU()
        (4): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
      )
      (pool): TargetPooling()
    )
    (7): GCNModel(
      (layers): ModuleList(
        (0): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
        (1): ReLU()
        (2): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
        (3): ReLU()
        (4): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
      )
      (pool): TargetPooling()
    )
    (8): GCNModel(
      (layers): ModuleList(
        (0): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
        (1): ReLU()
        (2): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
        (3): ReLU()
        (4): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
      )
      (pool): TargetPooling()
    )
    (9): GCNModel(
      (layers): ModuleList(
        (0): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
        (1): ReLU()
        (2): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
        (3): ReLU()
        (4): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
      )
      (pool): TargetPooling()
    )
    (10): GCNModel(
      (layers): ModuleList(
        (0): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
        (1): ReLU()
        (2): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
        (3): ReLU()
        (4): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
      )
      (pool): TargetPooling()
    )
    (11): GCNModel(
      (layers): ModuleList(
        (0): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
        (1): ReLU()
        (2): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
        (3): ReLU()
        (4): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
      )
      (pool): TargetPooling()
    )
    (12): GCNModel(
      (layers): ModuleList(
        (0): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
        (1): ReLU()
        (2): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
        (3): ReLU()
        (4): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
      )
      (pool): TargetPooling()
    )
    (13): GCNModel(
      (layers): ModuleList(
        (0): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
        (1): ReLU()
        (2): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
        (3): ReLU()
        (4): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
      )
      (pool): TargetPooling()
    )
    (14): GCNModel(
      (layers): ModuleList(
        (0): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
        (1): ReLU()
        (2): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
        (3): ReLU()
        (4): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
      )
      (pool): TargetPooling()
    )
    (15): GCNModel(
      (layers): ModuleList(
        (0): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
        (1): ReLU()
        (2): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
        (3): ReLU()
        (4): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
      )
      (pool): TargetPooling()
    )
    (16): GCNModel(
      (layers): ModuleList(
        (0): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
        (1): ReLU()
        (2): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
        (3): ReLU()
        (4): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
      )
      (pool): TargetPooling()
    )
    (17): GCNModel(
      (layers): ModuleList(
        (0): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
        (1): ReLU()
        (2): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
        (3): ReLU()
        (4): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
      )
      (pool): TargetPooling()
    )
    (18): GCNModel(
      (layers): ModuleList(
        (0): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
        (1): ReLU()
        (2): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
        (3): ReLU()
        (4): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
      )
      (pool): TargetPooling()
    )
    (19): GCNModel(
      (layers): ModuleList(
        (0): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
        (1): ReLU()
        (2): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
        (3): ReLU()
        (4): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
      )
      (pool): TargetPooling()
    )
    (20): GCNModel(
      (layers): ModuleList(
        (0): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
        (1): ReLU()
        (2): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
        (3): ReLU()
        (4): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
      )
      (pool): TargetPooling()
    )
    (21): GCNModel(
      (layers): ModuleList(
        (0): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
        (1): ReLU()
        (2): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
        (3): ReLU()
        (4): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
      )
      (pool): TargetPooling()
    )
    (22): GCNModel(
      (layers): ModuleList(
        (0): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
        (1): ReLU()
        (2): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
        (3): ReLU()
        (4): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
      )
      (pool): TargetPooling()
    )
    (23): GCNModel(
      (layers): ModuleList(
        (0): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
        (1): ReLU()
        (2): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
        (3): ReLU()
        (4): CompGCNConv(128, 128, comp=mult, bias=True, batchnorm=True, dropout=0.0)
      )
      (pool): TargetPooling()
    )
  )
)
03/19/2022 02:58:37 |  root |  INFO | Train info: {'1-chain': 1162489}
03/19/2022 02:58:37 |  root |  INFO | Val info: {'1-chain': 11843}
03/19/2022 02:58:38 |  root |  INFO | Test info: {'1-chain': 131450}
03/19/2022 03:03:02 | train |  INFO | Mean pos: 0.49879515171051025 | Mean neg: 0.5002400279045105
03/19/2022 03:03:02 | train |  INFO | Mean epoch loss: -0.0014449096051976085
